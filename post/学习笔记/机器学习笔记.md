---

typora-copy-images-to: img\机器学习

---



[b站视频链接](https://www.bilibili.com/video/BV164411b7dx)



## 监督学习

监督学习是已经知道数据的label，例如预测房价问题，给出了房子的面积和价格。

- 回归问题是预测连续值的输出，预测房价。

  ![image-20200922232432315](img/机器学习/image-20200922232432315.png)

- 分类问题是预测离散值输出，例如判断肿瘤是良性还是恶性。

  ![image-20200922232452094](img/机器学习/image-20200922232452094.png)



## 无监督学习

无监督学习是不知道数据具体的含义，比如给定一些数据，但是不知道它们具体的信息，无监督学习可以得到两个不同的聚类。

![image-20200922232542888](img/机器学习/image-20200922232542888.png)



## 线性回归

![image-20200925221455511](img/机器学习/image-20200925221455511.png)

线性回归是拟合一个直线，将训练数据尽可能分布到直线上。大多数只涉及到要给自变量和一个因变量称为一元线性回归，对于两个及以上的自变量且自变量和因变量是线性关系，称为多元线性回归。



## 代价函数

cost function，对于一般的回归问题，一般使用最小均方差来评估参数的好坏。

![image-20200925222847271](img/机器学习/image-20200925222847271.png)

**$\theta_0 = 0$**

![image-20200925225215790](img/机器学习/image-20200925225215790.png)

**$\theta_0 \neq 0$**

![image-20200925230206340](img/机器学习/image-20200925230206340.png)

![image-20200925230227443](img/机器学习/image-20200925230227443.png)



## 梯度下降

梯度下降的思路是为每个参数赋一个初值，然后预测使代价函数最小的方向，然后不断地调整参数，直到到达一个局部最优解。初值的不同可能会得到两个不同的结果。

![image-20200926193450504](img/机器学习/image-20200926193450504.png)

梯度下降在具体的执行时，每步计算需要同时更新所有的参数。

![image-20200926193952535](img/机器学习/image-20200926193952535.png)



梯度下降公式中有两个部分，学习率和偏导数。

偏导数，用来计算当前参数对应代价函数的斜率，导数为正则$\theta$减小，导数为负则$\theta$增大，通过这样的方式可以使整体向$\theta=0$收敛。

![image-20200926195243593](img/机器学习/image-20200926195243593.png)

学习率$\alpha$用来描述学习率，每次参数改变的步长。$\alpha$的大小不好确定，如果太小，则需要很多步才能收敛，如果太大，最后可能不会收敛甚至可能发散。

![image-20200926195756443](img/机器学习/image-20200926195756443.png)

当$\theta$处于局部最优解使，$\theta$的值将不再更新，因为偏导为0。

![image-20200926195859632](img/机器学习/image-20200926195859632.png)

这也说明了如果学习率$\alpha$不改变，参数也可能会收敛，假设偏导$> 0$，因为偏导一直在向在减小，所以每次的步长也会慢慢减小，所以$\alpha$不需要额外的减小。

![image-20200926200328203](img/机器学习/image-20200926200328203.png)



### 单元梯度下降



梯度下降每次更新的都需要进行偏导计算，这个偏导对应线性回归的代价函数。

![image-20200926202751517](img/机器学习/image-20200926202751517.png)

对代价函数求导的结果为：

![image-20200926202845222](img/机器学习/image-20200926202845222.png)

梯度下降的过程容易出现局部最优解：

![image-20200926193450504](img/机器学习/image-20200926193450504.png)

但是线性回归的代价函数，往往是一个凸函数。它总能收敛到全局最优。

![image-20200926203154017](img/机器学习/image-20200926203154017.png)

梯度下降过程的动图展示：

![](img/机器学习/image-20200926203154027.gif)

### 多元梯度下降

通常问题都会涉及到多个变量，例如房屋价格预测就包括，面积、房间个数、楼层、价格等

![image-20200927122907240](img/机器学习/image-20200927122907240.png)



因此代价函数就不再只包含一个变量，为了统一可以对常量引入变量$x_0=1$。

![image-20200927122848759](img/机器学习/image-20200927122848759.png)



虽然参数的个数增多，但是对每个参数求偏导时和单个参数类似。

![image-20200927123117176](img/机器学习/image-20200927123117176.png)



**特征缩放**

多个变量的度量不同，数字之间相差的大小也不同，如果可以将所有的特征变量缩放到大致相同范围，这样会减少梯度算法的迭代。

![image-20200927122159371](img/机器学习/image-20200927122159371.png)

特征缩放不一定非要落到[-1，1]之间，只要数据足够接近就可以。

![image-20200927122356465](img/机器学习/image-20200927122356465.png)

**均值归一化**

数据减去平均值然后除以范围，这样可以使得数据接近0值。

![image-20200927122645246](img/机器学习/image-20200927122645246.png)





### 学习率

学习率$\alpha$的大小会影响梯度算法的执行，太大可能会导致算法不收敛，太小会增加迭代的次数。

可以画出每次迭代的$J(\theta)$的变化，来判断当前算法执行的情况，然后选择合适的学习率。（调参开始...）

![image-20200927124215044](img/机器学习/image-20200927124215044.png)

![image-20200927124314735](img/机器学习/image-20200927124314735.png)





**Batch梯度下降**：每一步梯度下降，都需要遍历整个训练集样本。



## 矩阵和向量

一些数学计算转化为矩阵的形式，可以简化代码书写、提高效率、代码更容易理解。

![image-20200926215031531](img/机器学习/image-20200926215031531.png)

![image-20200926223409425](img/机器学习/image-20200926223409425.png)



**矩阵乘法不满足交换律：**

![image-20200926225453052](img/机器学习/image-20200926225453052.png)



**矩阵乘法满足结合律**：

![image-20200926225542850](img/机器学习/image-20200926225542850.png)

**单位矩阵**：

![image-20200926225634974](img/机器学习/image-20200926225634974.png)



**矩阵的逆**：

- 首先是方阵
- 不是所有的矩阵都有逆

![image-20200926225331312](img/机器学习/image-20200926225331312.png)

**转置矩阵**：

![image-20200926225407464](img/机器学习/image-20200926225407464.png)

## 正则方程

偏导等于0对应线性方程的最小值：

![image-20200927173714299](img/机器学习/image-20200927173714299.png)



利用线性代数的方法直接求解$\theta$。

![image-20200927173747699](img/机器学习/image-20200927173747699.png)

$\theta$的推导可以根据等式$X\theta=y$得到，X^TX$的目的是将矩阵转化为方阵，因为求矩阵的逆的前提是方阵。

矩阵可能存在不可逆的情况，这时可是删除一些不必要的特征，或使用正则化。

梯度下降和正则方程的优缺点：

![image-20200927174320125](img/机器学习/image-20200927174320125.png)