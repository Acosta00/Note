---

typora-copy-images-to: img\机器学习

---

[b站视频链接](https://www.bilibili.com/video/BV164411b7dx)



## 监督学习

监督学习是已经知道数据的label，例如预测房价问题，给出了房子的面积和价格。

- 回归问题是预测连续值的输出，例如预测房价。

  ![image-20200922232432315](img/机器学习/image-20200922232432315.png)

- 分类问题是预测离散值输出，例如判断肿瘤是良性还是恶性。

  ![image-20200922232452094](img/机器学习/image-20200922232452094.png)



## 无监督学习

无监督学习是不知道数据具体的含义，比如给定一些数据但不知道它们具体的信息，对于分类问题无监督学习可以得到多个不同的聚类，从而实现预测的功能。

![image-20200922232542888](img/机器学习/image-20200922232542888.png)



## 线性回归

![image-20200925221455511](img/机器学习/image-20200925221455511.png)

线性回归是拟合一个直线，将训练数据尽可能分布到直线上。只包含一个自变量、因变量称为一元线性回归，对于两个及以上的自变量且自变量和因变量是线性关系，称为多元线性回归。



## 代价函数

cost function，一般使用最小均方差来评估参数的好坏。

![image-20200925222847271](img/机器学习/image-20200925222847271.png)

**$\theta_0 = 0$**

![image-20200925225215790](img/机器学习/image-20200925225215790.png)

**$\theta_0 \neq 0$**

![image-20200925230206340](img/机器学习/image-20200925230206340.png)

![image-20200925230227443](img/机器学习/image-20200925230227443.png)



## 梯度下降

梯度下降，首先为每个参数赋一个初值，通过代价函数的梯度，然后不断地调整参数，最终得到一个局部最优解。初值的不同可能会得到两个不同的结果。

![image-20200926193450504](img/机器学习/image-20200926193450504.png)

梯度下降在具体的执行时，每一次更新需要同时更新所有的参数。

![image-20200926193952535](img/机器学习/image-20200926193952535.png)



梯度下降公式中有两个部分，学习率和偏导数。

偏导数，用来计算当前参数对应代价函数的斜率，导数为正则$\theta$减小，导数为负则$\theta$增大，通过这样的方式可以使整体向$\theta=0$收敛。

![image-20200926195243593](img/机器学习/image-20200926195243593.png)

$\alpha$用来描述学习率，即每次参数更新的步长。$\alpha$的大小不好确定，如果太小则需要很多步才能收敛，如果太大最后可能不会收敛甚至可能发散。

![image-20200926195756443](img/机器学习/image-20200926195756443.png)

当$\theta$处于局部最优解时，$\theta$的值将不再更新，因为偏导为0。

![image-20200926195859632](img/机器学习/image-20200926195859632.png)

这也说明了如果学习率$\alpha$不改变，参数也可能会收敛，假设偏导$> 0$，因为偏导一直在向在减小，所以每次的步长也会慢慢减小，所以$\alpha$不需要额外的减小。

![image-20200926200328203](img/机器学习/image-20200926200328203.png)



### 单元梯度下降



梯度下降每次更新的都需要进行偏导计算，这个偏导对应线性回归的代价函数。

![image-20200926202751517](img/机器学习/image-20200926202751517.png)

对代价函数求导的结果为：

![image-20200926202845222](img/机器学习/image-20200926202845222.png)

梯度下降的过程容易出现局部最优解：

![image-20200926193450504](img/机器学习/image-20200926193450504.png)

但是线性回归的代价函数，往往是一个凸函数。它总能收敛到全局最优。

![image-20200926203154017](img/机器学习/image-20200926203154017.png)

梯度下降过程的动图展示：

![](img/机器学习/image-20200926203154027.gif)

### 多元梯度下降

通常问题都会涉及到多个变量，例如房屋价格预测就包括，面积、房间个数、楼层、价格等

![image-20200927122907240](img/机器学习/image-20200927122907240.png)



因此代价函数就不再只包含一个变量，为了统一可以对常量引入变量$x_0=1$。

![image-20200927122848759](img/机器学习/image-20200927122848759.png)



虽然参数的个数增多，但是对每个参数求偏导时和单个参数类似。

![image-20200927123117176](img/机器学习/image-20200927123117176.png)



**特征缩放**

多个变量的度量不同，数字之间相差的大小也不同，如果可以将所有的特征变量缩放到大致相同范围，这样会减少梯度算法的迭代。

![image-20200927122159371](img/机器学习/image-20200927122159371.png)

特征缩放不一定非要落到[-1，1]之间，只要数据足够接近就可以。

![image-20200927122356465](img/机器学习/image-20200927122356465.png)

**均值归一化**

数据减去平均值然后除以范围，这样可以使得数据接近0值。

![image-20200927122645246](img/机器学习/image-20200927122645246.png)





### 学习率

学习率$\alpha$的大小会影响梯度算法的执行，太大可能会导致算法不收敛，太小会增加迭代的次数。

可以画出每次迭代的$J(\theta)$的变化，来判断当前算法执行的情况，然后选择合适的学习率。（调参开始...）

![image-20200927124215044](img/机器学习/image-20200927124215044.png)

![image-20200927124314735](img/机器学习/image-20200927124314735.png)





**Batch梯度下降**：每一步梯度下降，都需要遍历整个训练集样本。



## 矩阵和向量

一些数学计算转化为矩阵的形式，可以简化代码书写、提高效率、代码更容易理解。

![image-20200926215031531](img/机器学习/image-20200926215031531.png)

![image-20200926223409425](img/机器学习/image-20200926223409425.png)



**矩阵乘法不满足交换律：**

![image-20200926225453052](img/机器学习/image-20200926225453052.png)



**矩阵乘法满足结合律**：

![image-20200926225542850](img/机器学习/image-20200926225542850.png)

**单位矩阵**：

![image-20200926225634974](img/机器学习/image-20200926225634974.png)



**矩阵的逆**：

- 首先是方阵
- 不是所有的矩阵都有逆

![image-20200926225331312](img/机器学习/image-20200926225331312.png)

**转置矩阵**：

![image-20200926225407464](img/机器学习/image-20200926225407464.png)

## 正则方程

偏导等于0对应线性方程的最小值：

![image-20200927173714299](img/机器学习/image-20200927173714299.png)



利用线性代数的方法直接求解$\theta$。

![image-20200927173747699](img/机器学习/image-20200927173747699.png)

$\theta$的推导可以根据等式$X\theta=y$得到，$X^TX$的目的是将矩阵转化为方阵，因为求矩阵的逆的前提是方阵。

矩阵可能存在 不可逆的情况，这时可是删除一些不必要的特征，或使用正则化。

梯度下降和正则方程的优缺点：

![image-20200927174320125](img/机器学习/image-20200927174320125.png)



## 分类问题

对于分类的问题，一般不适用线性回归进行预测：使用线性回归可能会造成很大的误差，即使样本的标签值为0、1，线性回归输出值也会存在>1和小于0的情况，不太符合实际。

如果对于一个均匀的数据，使用线性回归，选取0.5作为分界线，可能会得到一个比较准确的模型，但是如果数据不太均匀就会存在很大的误差。

![image-20201002134249446](img/机器学习/image-20201002134249446.png)

![image-20201002134545596](img/机器学习/image-20201002134545596.png)



### 逻辑回归

Logistic Regression

[激活函数](https://github.com/Sanzona/files/blob/master/py/sigmoid.py)

![image-20201002142636471](img/机器学习/image-20201002142636471.png)

利用激活函数值分布的特性，可以得到在参数$\theta$下输入x得到y=1、0的概率。

![image-20201002144146860](img/机器学习/image-20201002144146860.png)



### 决策界限

决策边界是假设函数的一个属性，取决于函数的参数，而不是数据集。

根据sigmoid的性质，以x=0，y=0.5作为判断的界限，当即$\theta^Tx >=0$，y=1；$\theta^Tx <0$，y=0。

![image-20201002151014707](img/机器学习/image-20201002151014707.png)                

![image-20201002151544351](img/机器学习/image-20201002151544351.png)

![image-20201002151951691](img/机器学习/image-20201002151951691.png)



### 代价函数

对于函数$f(x)=\frac{1}{1+e^{-x}}$，如果使用类似线性回归的代价函数$\Sigma(h(x)-y)^2$，将得到一个非凸函数，这样就不能使用梯度下降的方法求解全局最优解。

![image-20201002155109200](img/机器学习/image-20201002155109200.png)

逻辑回归一般使用对数函数作为代价函数：

首先对于分类函数来说，他的输出值范围为[0,1]，得到的对数图像如下：

![image-20201002155225235](img/机器学习/image-20201002155225235.png)

当评估模型参数对y=1（恶性肿瘤）进行预测的好坏时，如果实际为恶性，预测值也为1（恶性），此时的代价为0；如果实际为恶性，预测为0（良性），此时的代价为$+\infty$，这时代价函数就很好的评估了参数$\theta$的表现。

![image-20201002160155129](img/机器学习/image-20201002160155129.png)

同样对于y=0（良性肿瘤）的代价函数为：

![image-20201002160126721](img/机器学习/image-20201002160126721.png)



y的取值只有0、1，可以将上面两个函数合成一个，评估当前参数的$J(\theta)$为：

![image-20201002193349426](img/机器学习/image-20201002193349426.png)

在确定代价函数之后的任务是，如何最小化代价函数，因为代价函数是凸的，所以可以使用梯度下降求解。

![image-20201002193549679](img/机器学习/image-20201002193549679.png)

![image-20201002193728758](img/机器学习/image-20201002193728758.png)



虽然求偏导之后，$\theta$更新的形式和线性回归类似，但是他们本质不同，因为$h_\theta(x)$完全不一样。

[具体的偏导推导过程](https://www.cnblogs.com/zhongmiaozhimen/p/6155093.html)：

![image-20201002193927240](img/机器学习/image-20201002193927240.png)



### 多元分类

![image-20201002200132671](img/机器学习/image-20201002200132671.png)

对每个特征单独训练，在做预测的时候，取三个分类器结果最大的。

![image-20201002200303264](img/机器学习/image-20201002200303264.png)



## 过拟合

存在多个特征，但是数据很少，或者模型函数不合理，都会出现过拟合的现象。过拟合可能对样本数能够很好的解释，但是无法正确的预测新数据。

![image-20201002211447558](img/机器学习/image-20201002211447558.png)

![image-20201002211511303](img/机器学习/image-20201002211511303.png)



解决过拟合的方法：

![image-20201002211815208](img/机器学习/image-20201002211815208.png)

正则化处理过拟合问题：

在代价函数中加入正则项，通过lambda的来平衡拟合程度和参数的大小，$\theta$约大越容易出现过拟合的现象。

![image-20201009160444537](img/机器学习/image-20201009160444537.png)



如果lambda过大，导致$\theta \approx 0$，那么最终只剩下下$\theta_0$，图像将变成一个直线。

![image-20201009160357606](img/机器学习/image-20201009160357606.png)